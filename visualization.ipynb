{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      id  prediction\n",
       "0      0      0.7194\n",
       "1      1      0.8082\n",
       "2      2      0.2251\n",
       "3      3      0.7629\n",
       "4      4      0.2121\n",
       "..   ...         ...\n",
       "739  739      0.1221\n",
       "740  740      0.7481\n",
       "741  741      0.8730\n",
       "742  742      0.8904\n",
       "743  743      0.7026\n",
       "\n",
       "[744 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.7194</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.8082</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.2251</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.7629</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.2121</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>739</th>\n      <td>739</td>\n      <td>0.1221</td>\n    </tr>\n    <tr>\n      <th>740</th>\n      <td>740</td>\n      <td>0.7481</td>\n    </tr>\n    <tr>\n      <th>741</th>\n      <td>741</td>\n      <td>0.8730</td>\n    </tr>\n    <tr>\n      <th>742</th>\n      <td>742</td>\n      <td>0.8904</td>\n    </tr>\n    <tr>\n      <th>743</th>\n      <td>743</td>\n      <td>0.7026</td>\n    </tr>\n  </tbody>\n</table>\n<p>744 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test= pd.read_csv(\"/opt/ml/code/AKT/github/output/output_lgbm.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    " \n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def plot_head_map(mma, target_labels, source_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(mma, cmap=plt.cm.Blues)\n",
    " \n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(numpy.arange(mma.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(numpy.arange(mma.shape[0]) + 0.5, minor=False)\n",
    " \n",
    "    ax.set_xlim(0, int(mma.shape[1]))\n",
    "    ax.set_ylim(0, int(mma.shape[0]))\n",
    " \n",
    "    # want a more natural, table-like display\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.tick_top()\n",
    " \n",
    "    # source words -> column labels\n",
    "    ax.set_xticklabels(source_labels, minor=False)\n",
    "    # target words -> row labels\n",
    "    ax.set_yticklabels(target_labels, minor=False)\n",
    " \n",
    "    plt.xticks(rotation=45)\n",
    " \n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    " \n",
    " \n",
    "# column labels -> target words\n",
    "# row labels -> source words\n",
    " \n",
    "def read_alignment_matrix(f):\n",
    "    header = f.readline().strip().split('|||')\n",
    "    if header[0] == '':\n",
    "        return None, None, None, None\n",
    "    sid = int(header[0].strip())\n",
    "    # number of tokens in source and translation +1 for eos\n",
    "    src_count, trg_count = map(int, header[-1].split())\n",
    "    # source words\n",
    "    source_labels = header[3].decode('UTF-8').split()\n",
    "    # source_labels.append('</s>')\n",
    "    # target words\n",
    "    target_labels = header[1].decode('UTF-8').split()\n",
    "    target_labels.append('</s>')\n",
    " \n",
    "    mm = []\n",
    "    for r in range(trg_count):\n",
    "        alignment = map(float, f.readline().strip().split())\n",
    "        mm.append(alignment)\n",
    "    mma = numpy.array(mm)\n",
    "    return sid, mma, target_labels, source_labels\n",
    "\n",
    "    \n",
    "def read_plot_alignment_matrices(f, start=0):\n",
    "    attentions = json.load(f, encoding=\"utf-8\")\n",
    " \n",
    "    for idx, att in attentions.items():\n",
    "        if idx < start: continue\n",
    "        source_labels = att[\"source\"].split() + [\"SEQUENCE_END\"]\n",
    "        target_labels = att[\"translation\"].split()\n",
    "        att_list = att[\"attentions\"]\n",
    "        assert att_list[0][\"type\"] == \"simple\", \"Do not use this tool for multihead attention.\"\n",
    "        mma = numpy.array(att_list[0][\"value\"])\n",
    "        if mma.shape[0] == len(target_labels) + 1:\n",
    "            target_labels += [\"SEQUENCE_END\"]\n",
    " \n",
    "        plot_head_map(mma, target_labels, source_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PID_DATA(object):\n",
    "    def __init__(self, n_question,  seqlen):\n",
    "\n",
    "        self.seqlen = seqlen\n",
    "        self.n_question = n_question\n",
    "\n",
    "    def load_data(self, f_data):\n",
    "        # f_data = open(path, 'r')\n",
    "        q_data = []\n",
    "        qa_data = []\n",
    "        p_data = []\n",
    "        for idx, line in enumerate(f_data):\n",
    "            Q = line[1]\n",
    "            P = line[2]\n",
    "            A = line[3]\n",
    "            # A = A[:-1]\n",
    "\n",
    "            # start split the data\n",
    "            n_split = 1\n",
    "            # print('len(Q):',len(Q))\n",
    "            if len(Q) > self.seqlen:\n",
    "                n_split = math.floor(len(Q) / self.seqlen)\n",
    "                if len(Q) % self.seqlen:\n",
    "                    n_split = n_split + 1\n",
    "            # print('n_split:',n_split)\n",
    "            for k in range(n_split):\n",
    "                question_sequence = []\n",
    "                problem_sequence = []\n",
    "                answer_sequence = []\n",
    "                if k == n_split - 1:\n",
    "                    endINdex = len(A)\n",
    "                else:\n",
    "                    endINdex = (k+1) * self.seqlen\n",
    "                for i in range(k * self.seqlen, endINdex):\n",
    "                    if Q[i]>0:\n",
    "                        Xindex = int(Q[i]) + int(A[i]) * self.n_question\n",
    "                        question_sequence.append(int(Q[i]))\n",
    "                        problem_sequence.append(int(P[i]))\n",
    "                        answer_sequence.append(Xindex)\n",
    "                    else:\n",
    "                        print(Q[i])\n",
    "                q_data.append(question_sequence)\n",
    "                qa_data.append(answer_sequence)\n",
    "                p_data.append(problem_sequence)\n",
    "\n",
    "        # f_data.close()\n",
    "        ### data: [[],[],[],...] <-- set_max_seqlen is used\n",
    "        # convert data into ndarrays for better speed during training\n",
    "        q_dataArray = np.zeros((len(q_data), self.seqlen))\n",
    "        for j in range(len(q_data)):\n",
    "            dat = q_data[j]\n",
    "            q_dataArray[j, :len(dat)] = dat\n",
    "\n",
    "        qa_dataArray = np.zeros((len(qa_data), self.seqlen))\n",
    "        for j in range(len(qa_data)):\n",
    "            dat = qa_data[j]\n",
    "            qa_dataArray[j, :len(dat)] = dat\n",
    "\n",
    "        p_dataArray = np.zeros((len(p_data), self.seqlen))\n",
    "        for j in range(len(p_data)):\n",
    "            dat = p_data[j]\n",
    "            p_dataArray[j, :len(dat)] = dat\n",
    "        return q_dataArray, qa_dataArray, p_dataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e4c6623024af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m checkpoint = torch.load(os.path.join(\n\u001b[1;32m      4\u001b[0m     'model', args.model, args.save, 'test') + '_'+str(20))\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model(args)\n",
    "\n",
    "checkpoint = torch.load(os.path.join(\n",
    "    'model', args.model, args.save, 'test') + '_'+str(20))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose_data_model = {'akt'}\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from utils import model_isPid_type\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def test(net, params, optimizer, q_data, qa_data, pid_data, label):\n",
    "    # dataArray: [ array([[],[],..])] Shape: (3633, 200)\n",
    "    pid_flag, model_type = model_isPid_type(params.model)\n",
    "    net.eval()\n",
    "    N = int(math.ceil(float(len(q_data)) / float(params.batch_size)))\n",
    "    q_data = q_data.T  # Shape: (200,3633)\n",
    "    qa_data = qa_data.T  # Shape: (200,3633)\n",
    "    if pid_flag:\n",
    "        pid_data = pid_data.T\n",
    "    seq_num = q_data.shape[1]\n",
    "    pred_list = []\n",
    "    target_list = []\n",
    "\n",
    "    count = 0\n",
    "    true_el = 0\n",
    "    element_count = 0\n",
    "    for idx in range(N):\n",
    "\n",
    "        q_one_seq = q_data[:, idx*params.batch_size:(idx+1)*params.batch_size]\n",
    "        if pid_flag:\n",
    "            pid_one_seq = pid_data[:, idx *\n",
    "                                   params.batch_size:(idx+1) * params.batch_size]\n",
    "        input_q = q_one_seq[:, :]  # Shape (seqlen, batch_size)\n",
    "        qa_one_seq = qa_data[:, idx *\n",
    "                             params.batch_size:(idx+1) * params.batch_size]\n",
    "        input_qa = qa_one_seq[:, :]  # Shape (seqlen, batch_size)\n",
    "\n",
    "        # print 'seq_num', seq_num\n",
    "        if model_type in transpose_data_model:\n",
    "            # Shape (seqlen, batch_size)\n",
    "            input_q = np.transpose(q_one_seq[:, :])\n",
    "            # Shape (seqlen, batch_size)\n",
    "            input_qa = np.transpose(qa_one_seq[:, :])\n",
    "            target = np.transpose(qa_one_seq[:, :])\n",
    "            if pid_flag:\n",
    "                input_pid = np.transpose(pid_one_seq[:, :])\n",
    "        else:\n",
    "            input_q = (q_one_seq[:, :])  # Shape (seqlen, batch_size)\n",
    "            input_qa = (qa_one_seq[:, :])  # Shape (seqlen, batch_size)\n",
    "            target = (qa_one_seq[:, :])\n",
    "            if pid_flag:\n",
    "                input_pid = (pid_one_seq[:, :])\n",
    "        target = (target - 1) / params.n_question\n",
    "        target_1 = np.floor(target)\n",
    "        #target = np.random.randint(0,2, size = (target.shape[0],target.shape[1]))\n",
    "\n",
    "        input_q = torch.from_numpy(input_q).long().to(device)\n",
    "        input_qa = torch.from_numpy(input_qa).long().to(device)\n",
    "        target = torch.from_numpy(target_1).float().to(device)\n",
    "        if pid_flag:\n",
    "            input_pid = torch.from_numpy(input_pid).long().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if pid_flag:\n",
    "                loss, pred, ct = net(input_q, input_qa, target, input_pid)\n",
    "            else:\n",
    "                loss, pred, ct = net(input_q, input_qa, target)\n",
    "        pred = pred.cpu().numpy()  # (seqlen * batch_size, 1)\n",
    "        true_el += ct.cpu().numpy()\n",
    "        #target = target.cpu().numpy()\n",
    "        if (idx + 1) * params.batch_size > seq_num:\n",
    "            real_batch_size = seq_num - idx * params.batch_size\n",
    "            count += real_batch_size\n",
    "        else:\n",
    "            count += params.batch_size\n",
    "\n",
    "        # correct: 1.0; wrong 0.0; padding -1.0\n",
    "        target = target_1.reshape((-1,))\n",
    "        nopadding_index = np.flatnonzero(target >= -0.9)\n",
    "        nopadding_index = nopadding_index.tolist()\n",
    "        pred_nopadding = pred[nopadding_index]\n",
    "        target_nopadding = target[nopadding_index]\n",
    "\n",
    "        element_count += pred_nopadding.shape[0]\n",
    "        # print avg_loss\n",
    "        pred_list.append(pred_nopadding)\n",
    "        target_list.append(target_nopadding)\n",
    "\n",
    "    assert count == seq_num, \"Seq not matching\"\n",
    "\n",
    "    all_pred = np.concatenate(pred_list, axis=0)\n",
    "    # all_target = np.concatenate(target_list, axis=0)\n",
    "    # loss = binaryEntropy(all_target, all_pred)\n",
    "    # auc = compute_auc(all_target, all_pred)\n",
    "    # accuracy = compute_accuracy(all_target, all_pred)\n",
    "\n",
    "    # return loss, accuracy, auc\n",
    "    return all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9455"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "n_q = preprocess.args.n_questions\n",
    "n_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "len(test_pid[test_qa_data<0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dat = PID_DATA(n_question=n_q,\n",
    "                       seqlen=200)\n",
    "test_q_data, test_qa_data, test_pid = dat.load_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'EasyDict' object has no attribute 'embed_l'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a891d93eb596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mqa_embed_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_q\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'EasyDict' object has no attribute 'embed_l'"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "qa_embed_diff = nn.Embedding(2 * n_q + 1, args.embed_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([12522., 12523., 12524., 12525.,  3071., 13533., 13534., 13535.,\n",
       "        4081.,  4082., 13538.,  3728., 13184., 13185., 13186., -5723.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "test_qa_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9455"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "dat.n_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-ae3f7e1041a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_q_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_qa_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-c3aef26c2213>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net, params, optimizer, q_data, qa_data, pid_data, label)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpid_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_qa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_pid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_qa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/AKT/akt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q_data, qa_data, target, pid_data)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Pass to the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# output shape BS,seqlen,d_model or d_model//2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0md_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_embed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_embed_data\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 211x512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mconcat_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_embed_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/AKT/akt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q_embed_data, qa_embed_data)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks_1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# encode qas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mflag_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks_2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/AKT/akt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mask, query, key, values, apply_pos)\u001b[0m\n\u001b[1;32m    203\u001b[0m         nopeek_mask = np.triu(\n\u001b[1;32m    204\u001b[0m             np.ones((1, 1, seqlen, seqlen)), k=mask).astype('uint8')\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnopeek_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# If 0, zero-padding is needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# Calls block.masked_attn_head.forward() method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "all_pred=test(model, args, None, test_q_data, test_qa_data, test_pid, label='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-2428231c1de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_q_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_qa_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_pid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-c3aef26c2213>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net, params, optimizer, q_data, qa_data, pid_data, label)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#target = np.random.randint(0,2, size = (target.shape[0],target.shape[1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0minput_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0minput_qa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_qa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "all_pred=test(model, args, None, valid_q_data, valid_qa_data, valid_pid, label='Valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(200, 5036)"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "valid_pid.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nakt\n"
     ]
    }
   ],
   "source": [
    "a,b=model_isPid_type(args.model)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(200, 5036)\n"
     ]
    }
   ],
   "source": [
    "N = int(math.ceil(float(len(valid_q_data)) / float(24)))\n",
    "valid_q_data = valid_q_data.T  # Shape: (200,3633)\n",
    "valid_qa_data = valid_qa_data.T  # Shape: (200,3633)\n",
    "print(valid_q_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(24, 200)"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "q_one_seq = valid_q_data[:, 0:24]\n",
    "q_one_seq.shape\n",
    "input_q = np.transpose(q_one_seq[:, :])\n",
    "input_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(200, 24)"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "input_q = np.transpose(input_q[:, :])\n",
    "input_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,  ..., 9176, 9177, 9178])"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "torch.from_numpy(input_q).long().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_q = torch.from_numpy(input_q).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, params, optimizer, q_data, qa_data, pid_data, label):\n",
    "    # dataArray: [ array([[],[],..])] Shape: (3633, 200)\n",
    "    pid_flag, model_type = model_isPid_type(params.model)\n",
    "    net.eval()\n",
    "    N = int(math.ceil(float(len(q_data)) / float(params.batch_size)))\n",
    "    q_data = q_data.T  # Shape: (200,3633)\n",
    "    qa_data = qa_data.T  # Shape: (200,3633)\n",
    "    if pid_flag:\n",
    "        pid_data = pid_data.T\n",
    "    seq_num = q_data.shape[1]\n",
    "    pred_list = []\n",
    "    target_list = []\n",
    "\n",
    "    count = 0\n",
    "    true_el = 0\n",
    "    element_count = 0\n",
    "    for idx in range(N):\n",
    "\n",
    "        q_one_seq = q_data[:, idx*params.batch_size:(idx+1)*params.batch_size]\n",
    "        if pid_flag:\n",
    "            pid_one_seq = pid_data[:, idx *\n",
    "                                   params.batch_size:(idx+1) * params.batch_size]\n",
    "        input_q = q_one_seq[:, :]  # Shape (seqlen, batch_size)\n",
    "        qa_one_seq = qa_data[:, idx *\n",
    "                             params.batch_size:(idx+1) * params.batch_size]\n",
    "        input_qa = qa_one_seq[:, :]  # Shape (seqlen, batch_size)\n",
    "\n",
    "        # print 'seq_num', seq_num\n",
    "        if model_type in transpose_data_model:\n",
    "            # Shape (seqlen, batch_size)\n",
    "            input_q = np.transpose(q_one_seq[:, :])\n",
    "            # Shape (seqlen, batch_size)\n",
    "            input_qa = np.transpose(qa_one_seq[:, :])\n",
    "            target = np.transpose(qa_one_seq[:, :])\n",
    "            if pid_flag:\n",
    "                input_pid = np.transpose(pid_one_seq[:, :])\n",
    "        else:\n",
    "            input_q = (q_one_seq[:, :])  # Shape (seqlen, batch_size)\n",
    "            input_qa = (qa_one_seq[:, :])  # Shape (seqlen, batch_size)\n",
    "            target = (qa_one_seq[:, :])\n",
    "            if pid_flag:\n",
    "                input_pid = (pid_one_seq[:, :])\n",
    "        target = (target - 1) / params.n_question\n",
    "        target_1 = np.floor(target)\n",
    "        #target = np.random.randint(0,2, size = (target.shape[0],target.shape[1]))\n",
    "        print(input_q.shape)\n",
    "        input_q = torch.from_numpy(input_q).long().to(device)\n",
    "        input_qa = torch.from_numpy(input_qa).long().to(device)\n",
    "        target = torch.from_numpy(target_1).float().to(device)\n",
    "        if pid_flag:\n",
    "            input_pid = torch.from_numpy(input_pid).long().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if pid_flag:\n",
    "                loss, pred, ct = net(input_q, input_qa, target, input_pid)\n",
    "            else:\n",
    "                loss, pred, ct = net(input_q, input_qa, target)\n",
    "        pred = pred.cpu().numpy()  # (seqlen * batch_size, 1)\n",
    "        true_el += ct.cpu().numpy()\n",
    "        #target = target.cpu().numpy()\n",
    "        if (idx + 1) * params.batch_size > seq_num:\n",
    "            real_batch_size = seq_num - idx * params.batch_size\n",
    "            count += real_batch_size\n",
    "        else:\n",
    "            count += params.batch_size\n",
    "\n",
    "        # correct: 1.0; wrong 0.0; padding -1.0\n",
    "        target = target_1.reshape((-1,))\n",
    "        nopadding_index = np.flatnonzero(target >= -0.9)\n",
    "        nopadding_index = nopadding_index.tolist()\n",
    "        pred_nopadding = pred[nopadding_index]\n",
    "        target_nopadding = target[nopadding_index]\n",
    "\n",
    "        element_count += pred_nopadding.shape[0]\n",
    "        # print avg_loss\n",
    "        pred_list.append(pred_nopadding)\n",
    "        target_list.append(target_nopadding)\n",
    "\n",
    "    assert count == seq_num, \"Seq not matching\"\n",
    "\n",
    "    all_pred = np.concatenate(pred_list, axis=0)\n",
    "    all_target = np.concatenate(target_list, axis=0)\n",
    "    loss = binaryEntropy(all_target, all_pred)\n",
    "    auc = compute_auc(all_target, all_pred)\n",
    "    accuracy = compute_accuracy(all_target, all_pred)\n",
    "\n",
    "    return loss, accuracy, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(24, 5036)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Input, output and indices must be on the current device",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-43311d4dbc8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_q_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_qa_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_pid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-123-b5c75b22ac7f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net, params, optimizer, q_data, qa_data, pid_data, label)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpid_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_qa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_pid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_qa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/AKT/akt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q_data, qa_data, target, pid_data)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpid_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Batch First\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mq_embed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_data\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# BS, seqlen,  d_model# c_ct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseparate_qa\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# BS, seqlen, d_model #f_(ct,rt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"
     ]
    }
   ],
   "source": [
    "args.batch_size=24\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss,acc,auc=test(model, args, None, valid_q_data, valid_qa_data, valid_pid, label='Valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_dataset(params, file_name, test_q_data, test_qa_data, test_pid,  best_epoch):\n",
    "    print(\"\\n\\nStart testing ......................\\n Best epoch:\", best_epoch)\n",
    "    model = load_model(params)\n",
    "\n",
    "    checkpoint = torch.load(os.path.join(\n",
    "        'model', params.model, params.save, file_name) + '_'+str(best_epoch))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    test_loss, test_accuracy, test_auc = test(\n",
    "        model, params, None, test_q_data, test_qa_data, test_pid, label='Test')\n",
    "    print(\"\\ntest_auc\\t\", test_auc)\n",
    "    print(\"test_accuracy\\t\", test_accuracy)\n",
    "    print(\"test_loss\\t\", test_loss)\n",
    "\n",
    "    # Now Delete all the models\n",
    "    path = os.path.join('model', params.model, params.save,  file_name) + '_*'\n",
    "    for i in glob.glob(path):\n",
    "        os.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "import torch\n",
    "\n",
    "data_dir = '/opt/ml/input/data/train_dataset'\n",
    "file_name = 'train_all.csv'\n",
    "test_file_name = 'test_data.csv'\n",
    "\n",
    "config = {}\n",
    "# ì„¤ì •\n",
    "config['data_dir'] = '/opt/ml/input/data/train_dataset'\n",
    "config['seed'] = 42\n",
    "config['device'] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config['max_iter'] = 300\n",
    "config['optim'] = 'adam'\n",
    "config['maxgradnorm'] = -1\n",
    "config['final_fc_dim'] = 512\n",
    "config['d_model'] = 256\n",
    "config['d_ff'] = 1024\n",
    "config['dropout'] = 0.05\n",
    "config['n_block'] = 1\n",
    "config['n_head'] = 8\n",
    "config['kq_same'] = 1\n",
    "config['l2'] = 1e-5\n",
    "config['q_embed_dim'] = 50\n",
    "config['qa_embed_dim'] = 256\n",
    "config['memory_size'] = 50\n",
    "config['init_std'] = 0.1\n",
    "config['hidden_dim'] = 512\n",
    "config['lamda_r'] = 0.1\n",
    "config['lamda_w1'] = 0.1\n",
    "config['lamda_w2'] = 0.1\n",
    "config['model'] = 'akt_pid'\n",
    "config['n_question'] = preprocess.args.n_questions\n",
    "config['n_pid'] = preprocess.args.n_tag\n",
    "config['seqlen'] = 200\n",
    "config['save'] = 'asset'\n",
    "config['load'] = 'asset'\n",
    "config['asset_dir'] = 'asset'\n",
    "config['model_dir'] = 'models'\n",
    "config['model_name'] = 'model.pt'\n",
    "config['output_dir'] = 'output'\n",
    "    \n",
    "args = easydict.EasyDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:61: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "setSeeds(42)\n",
    "\n",
    "preprocess = Preprocess(args)\n",
    "preprocess.load_test_data(test_file_name)\n",
    "\n",
    "test_data = preprocess.get_test_data()\n",
    "# train_data, valid_data = preprocess.split_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9455"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "n_q = preprocess.args.n_questions\n",
    "n_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "N = int(math.ceil(float(len(test_q_data)) / float(24)))\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import model_isPid_type\n",
    "def test(net, params, optimizer, q_data, qa_data, pid_data, label):\n",
    "    # dataArray: [ array([[],[],..])] Shape: (3633, 200)\n",
    "    pid_flag, model_type = model_isPid_type(params.model)\n",
    "    net.eval()\n",
    "    N = int(math.ceil(float(len(q_data)) / float(params.batch_size)))\n",
    "    q_data = q_data.T  # Shape: (200,3633)\n",
    "    qa_data = qa_data.T  # Shape: (200,3633)\n",
    "    if pid_flag:\n",
    "        pid_data = pid_data.T\n",
    "    seq_num = q_data.shape[1]\n",
    "    pred_list = []\n",
    "    target_list = []\n",
    "\n",
    "    count = 0\n",
    "    true_el = 0\n",
    "    element_count = 0\n",
    "    for idx in range(N):\n",
    "\n",
    "        q_one_seq = q_data[:, idx*params.batch_size:(idx+1)*params.batch_size]\n",
    "        if pid_flag:\n",
    "            pid_one_seq = pid_data[:, idx *\n",
    "                                   params.batch_size:(idx+1) * params.batch_size]\n",
    "        input_q = q_one_seq[:, :]  # Shape (seqlen, batch_size)\n",
    "        qa_one_seq = qa_data[:, idx *\n",
    "                             params.batch_size:(idx+1) * params.batch_size]\n",
    "        input_qa = qa_one_seq[:, :]  # Shape (seqlen, batch_size)\n",
    "\n",
    "        # print 'seq_num', seq_num\n",
    "        if model_type in transpose_data_model:\n",
    "            # Shape (seqlen, batch_size)\n",
    "            input_q = np.transpose(q_one_seq[:, :])\n",
    "            # Shape (seqlen, batch_size)\n",
    "            input_qa = np.transpose(qa_one_seq[:, :])\n",
    "            target = np.transpose(qa_one_seq[:, :])\n",
    "            if pid_flag:\n",
    "                input_pid = np.transpose(pid_one_seq[:, :])\n",
    "        else:\n",
    "            input_q = (q_one_seq[:, :])  # Shape (seqlen, batch_size)\n",
    "            input_qa = (qa_one_seq[:, :])  # Shape (seqlen, batch_size)\n",
    "            target = (qa_one_seq[:, :])\n",
    "            if pid_flag:\n",
    "                input_pid = (pid_one_seq[:, :])\n",
    "        target = (target - 1) / params.n_question\n",
    "        target_1 = np.floor(target)\n",
    "        #target = np.random.randint(0,2, size = (target.shape[0],target.shape[1]))\n",
    "        print(input_q.shape)\n",
    "        input_q = torch.from_numpy(input_q).long().to(device)\n",
    "        input_qa = torch.from_numpy(input_qa).long().to(device)\n",
    "        target = torch.from_numpy(target_1).float().to(device)\n",
    "        if pid_flag:\n",
    "            input_pid = torch.from_numpy(input_pid).long().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if pid_flag:\n",
    "                loss, pred, ct = net(input_q, input_qa, target, input_pid)\n",
    "            else:\n",
    "                loss, pred, ct = net(input_q, input_qa, target)\n",
    "        pred = pred.cpu().numpy()  # (seqlen * batch_size, 1)\n",
    "        true_el += ct.cpu().numpy()\n",
    "        #target = target.cpu().numpy()\n",
    "        if (idx + 1) * params.batch_size > seq_num:\n",
    "            real_batch_size = seq_num - idx * params.batch_size\n",
    "            count += real_batch_size\n",
    "        else:\n",
    "            count += params.batch_size\n",
    "\n",
    "        # correct: 1.0; wrong 0.0; padding -1.0\n",
    "        target = target_1.reshape((-1,))\n",
    "        # nopadding_index = np.flatnonzero(target >= -0.9)\n",
    "        nopadding_index = np.flatnonzero(target == -1)\n",
    "        nopadding_index = nopadding_index.tolist()\n",
    "        pred_nopadding = pred[nopadding_index]\n",
    "        # pred_nopadding = pred\n",
    "        # target_nopadding = target[nopadding_index]\n",
    "\n",
    "        # element_count += pred_nopadding.shape[0]\n",
    "        # print avg_loss\n",
    "        pred_list.append(pred_nopadding)\n",
    "        # target_list.append(target_nopadding)\n",
    "\n",
    "    assert count == seq_num, \"Seq not matching\"\n",
    "\n",
    "\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "344400"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "test_qa.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1722, 200)"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "test_qa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1722"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "len(test_qa<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 72 into shape (1722,200)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-cdb5f2d945f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1722\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    299\u001b[0m            [5, 6]])\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 72 into shape (1722,200)"
     ]
    }
   ],
   "source": [
    "pred_test=np.reshape(pred, (1722, 200))\n",
    "len(pred_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n",
      "(1, 200)\n"
     ]
    }
   ],
   "source": [
    "args.batch_size=1\n",
    "# transpose_data_model={'akt'}\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pred= test(model, args, None, test_q_data, test_qa, test_pid, label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "len(pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "344400"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "all_pred = np.concatenate(pred, axis=0)\n",
    "all_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "preds=all_pred[all_qa<0]\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "writing prediction : output/output_akt.csv\n"
     ]
    }
   ],
   "source": [
    "write_path = os.path.join(args.output_dir, \"output_akt.csv\")\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)    \n",
    "with open(write_path, 'w', encoding='utf8') as w:\n",
    "    print(\"writing prediction : {}\".format(write_path))\n",
    "    w.write(\"id,prediction\\n\")\n",
    "    for id, p in enumerate(preds):\n",
    "        w.write('{},{}\\n'.format(id,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "344400"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "all_qa = np.concatenate(test_qa_data, axis=0)\n",
    "all_qa.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([13690., 13691.,  4237.,  4238.,  4240.,  4241.,  4239.,  4263.,\n",
       "        4264.,  4265.,  4266.,  4268.,  4269.,  4267., 13781.,  4327.,\n",
       "        4328., 13784.,  4330.,  4331.,  4332., 13795., 13796., 13797.,\n",
       "       13798., 13799.,  4345.,  4346., 13840., 13841.,  4387.,  4388.,\n",
       "       13844.,  4390.,  4391., 10556., 10557., 10558., 10559., 10560.,\n",
       "       10561., 10562., 10563., 10564., 10565., 10566., 10567., 10568.,\n",
       "       10569., 10570., 10571., 10572., 10573., 10574., 10575., 10576.,\n",
       "       10577., 10547., 10548., 10549., 10550., 10551., 10552., 10553.,\n",
       "       10578., 10579.,  1125., 10581., 10582., 10583., 10584., 10585.,\n",
       "       10586., 10587., 10588., 10589., 10590., 10591., 10592., 10593.,\n",
       "       10594., 10595., 10596., 10597.,  1143., 10599., 10600., 10601.,\n",
       "       10608.,  1154., 10610., 10611., 10612., 10613., 10614., 10615.,\n",
       "       10554., 10555., 10602., 10603., 10604., 10605., 10606., 10607.,\n",
       "        4660., 14116., 14117., 14118.,  4664.,  4665., 14121., 10712.,\n",
       "       10713.,  1259., 10715.,  1261., 10717., 10718., 10719., 10720.,\n",
       "       10721., 10722., 10723., 10706.,  1252., 10708.,  1254., 10710.,\n",
       "       10711., 13826., 13827., 13828., 13829.,  4375., 13831.,  4377.,\n",
       "       13840., 13841.,  4387.,  4388., 13844., 13845.,  4391., 10724.,\n",
       "       10725.,  1271.,  1272.,  1273.,  1274., 10730., 10731., 10732.,\n",
       "       10733., 10734., 10735., 13620., 13621.,  4167., 13623., 13624.,\n",
       "        4413.,  4414.,  4415.,  4416.,  4417.,  4418.,  4419.,  4170.,\n",
       "       13626., 13627., 13628.,  4174., 10448., 10449., 10450., 10451.,\n",
       "       10452., 13630., 13631., 13632., 13633., 13634.,  4180., 10458.,\n",
       "       10459., 10460., 10461., 10462., 13564., 13565., 13566., 13567.,\n",
       "       13568.,  4114., 13570., 10478., 10479., 10480., 10481., 10482.])"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "test_qa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "len(test_qa_data[test_qa_data<0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qa=test_qa_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qa[test_qa<0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from utils import load_model\n",
    "model = load_model(args)\n",
    "\n",
    "checkpoint = torch.load(os.path.join(\n",
    "    'model', args.model, args.save, 'test') + '_'+str(20))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dat = PID_DATA(n_question=args.n_question,\n",
    "                       seqlen=200)\n",
    "test_q_data, test_qa_data, test_pid = dat.load_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([14382.,  4928.,  4929., 14499.,  5045.,  5046., 14502.,  5048.,\n",
       "        5049., 14392., 14393., 14394., 14395., 14396., 14397.,  4943.,\n",
       "       14406., 14407., 14408., 14409., 14410., 14411.,  4957.,  5055.,\n",
       "        5056., 14512., 14513.,  5059., 14413., 14414., 14415., 14416.,\n",
       "       14417., 14418.,  4964., -4490.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "test_qa_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([744])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tt=test_q_data[test_qa_data<0]\n",
    "# tt = torch.Tensor([tt]).long().to\n",
    "tt = torch.Tensor(tt).long().to(device)\n",
    "tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "tt_a = torch.Tensor([1]).long().to(device)\n",
    "tt_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_q=model.q_embed(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.1452e+00,  3.0688e-01, -2.9829e-02,  1.8261e-01, -3.2002e-01,\n",
       "          1.4787e+00,  2.0356e-01, -5.9621e-02, -4.5312e-01, -1.3840e+00,\n",
       "         -4.3169e-01, -2.9451e-01,  5.3537e-01,  1.3464e+00,  4.9898e-01,\n",
       "         -3.5237e-02,  6.8981e-01, -8.3332e-01,  2.5560e+00, -8.1424e-01,\n",
       "          2.7735e-01,  1.5562e+00, -6.8949e-01, -1.3928e+00,  8.3668e-01,\n",
       "          8.1764e-01, -4.0723e-01, -1.8605e+00, -1.7553e-01, -1.0286e+00,\n",
       "         -7.6797e-02,  2.4325e-01,  1.8028e+00,  7.5578e-02, -2.1831e-01,\n",
       "          1.6655e+00,  5.3284e-01, -3.1665e-01, -1.1740e+00,  4.3087e-01,\n",
       "         -7.7086e-01, -5.7961e-02,  2.1224e+00,  1.8020e+00, -1.5182e+00,\n",
       "         -4.1975e-01,  8.3604e-01, -2.2276e-01,  1.3628e+00, -1.0267e+00,\n",
       "         -3.3248e-01,  8.7700e-01,  6.6532e-02,  1.1426e-01, -1.9450e+00,\n",
       "         -1.7767e+00, -8.6300e-01,  4.9451e-01, -2.2454e-01, -1.1896e+00,\n",
       "         -1.5917e+00, -1.0079e+00,  1.4060e-01, -5.9011e-01, -1.4436e+00,\n",
       "          4.3004e-01, -7.3533e-01, -1.4671e+00,  1.9653e-01, -1.3502e-01,\n",
       "          4.7959e-02,  2.0962e+00,  1.6401e-02,  4.7679e-01,  1.5257e+00,\n",
       "          8.5583e-02, -1.8954e+00,  5.8121e-01, -2.1593e+00,  1.1919e+00,\n",
       "         -5.8814e-01, -9.8918e-02,  7.9217e-01,  4.3631e-02,  1.5532e+00,\n",
       "          4.4722e-01,  1.7704e+00, -1.2682e+00, -9.5607e-01, -1.7428e+00,\n",
       "          1.5760e+00,  7.5674e-01, -5.7997e-01, -2.8760e-02,  6.6856e-01,\n",
       "         -9.0989e-01,  8.0916e-01,  6.7162e-01,  9.0429e-01, -5.3899e-01,\n",
       "         -1.4629e+00,  9.4732e-02,  9.6713e-01,  3.8496e-01, -2.0837e+00,\n",
       "         -1.1934e+00, -1.5392e-01, -1.3546e+00,  5.9041e-01,  2.2006e-01,\n",
       "          8.9427e-01,  6.1624e-01,  1.6998e+00, -5.4234e-01,  2.0409e+00,\n",
       "         -9.0869e-01,  1.6353e+00, -2.7683e-01,  1.0135e+00, -3.4520e-02,\n",
       "         -1.3990e+00,  1.5248e+00,  1.9234e+00,  5.1419e-01,  2.0635e+00,\n",
       "          1.6070e+00, -7.7809e-01,  7.2040e-01, -8.1299e-01,  1.1246e+00,\n",
       "         -2.1189e+00,  1.9540e+00, -6.6169e-01, -1.3546e+00, -9.2309e-01,\n",
       "         -4.2785e-01,  2.4127e+00, -5.8654e-01,  3.6916e-01, -5.5231e-01,\n",
       "         -1.3902e-01, -6.1729e-01,  7.5741e-01, -6.8811e-01,  4.8146e-02,\n",
       "          1.2580e+00,  1.3474e+00,  2.9222e-01,  1.0385e+00, -1.2655e-02,\n",
       "         -1.9816e-01,  3.5335e-01, -8.2248e-01,  4.3423e-01, -1.2850e+00,\n",
       "         -1.9796e-03,  5.6454e-01,  1.0283e+00,  1.1027e+00, -8.1683e-01,\n",
       "         -1.8951e-01, -2.8416e+00,  6.0536e-01, -3.5026e-01, -8.9446e-01,\n",
       "          4.1040e-02, -4.4310e-02, -6.2202e-01, -1.1065e-01,  3.1021e-01,\n",
       "          1.3606e-01, -3.5479e-01,  1.1640e+00,  1.4811e+00, -9.9457e-01,\n",
       "         -5.6052e-01,  8.7239e-03, -1.2833e+00, -1.6023e+00, -2.8242e-01,\n",
       "         -4.9732e-01,  1.5633e-02,  5.0399e-01, -1.3309e+00, -1.7061e+00,\n",
       "          7.9739e-02,  1.9725e+00,  4.7303e-01,  9.1303e-01,  4.3163e-01,\n",
       "         -1.3653e+00,  2.3646e-01, -3.4581e-02, -1.7995e-01, -4.7074e-01,\n",
       "          1.6562e+00, -1.0362e+00,  2.0853e-01,  1.4525e+00, -4.4029e-01,\n",
       "          7.4816e-01, -1.9265e+00, -1.0566e+00, -2.8130e-01, -4.6306e-01,\n",
       "          1.5120e+00, -5.7900e-01,  1.0792e+00,  5.2404e-01, -5.8330e-02,\n",
       "         -1.3601e-02,  1.0147e+00,  4.4546e-01, -2.6097e-01,  1.0105e+00,\n",
       "         -6.5982e-01,  2.3947e-01, -2.5222e+00, -1.6875e+00,  3.6616e-01,\n",
       "         -1.7091e-01,  1.2378e-01,  8.1052e-01, -9.8369e-01, -6.4162e-01,\n",
       "          1.2698e+00, -7.0155e-02, -1.1763e+00,  2.8590e-01,  1.0664e+00,\n",
       "         -1.5506e+00, -8.2363e-01,  1.0088e+00, -1.7938e+00,  1.0566e+00,\n",
       "         -1.2302e+00,  1.9617e-01,  1.1781e+00,  2.4703e+00, -1.1294e+00,\n",
       "          1.3642e+00,  4.5769e-01,  1.0922e+00, -3.0343e-02, -9.3622e-01,\n",
       "         -5.2544e-02,  1.1416e+00, -2.5445e-01,  2.4282e-02, -1.1469e+00,\n",
       "          1.5047e+00, -1.7338e+00, -2.0342e-01, -8.6054e-01, -2.8179e-01,\n",
       "         -1.0057e+00]], device='cuda:0', grad_fn=<EmbeddingBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "tt_a=model.qa_embed(tt_a)\n",
    "tt_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.3594,  0.8826,  0.0990, -0.3442,  1.2487,  0.3218,  0.3483, -2.0587,\n",
       "         -1.4785, -2.2603,  0.5728, -1.8763, -0.3408,  0.3688,  1.8798, -0.0999,\n",
       "          1.5773, -1.9733,  1.4121, -0.4116,  2.8655,  1.0827,  0.2158, -1.4430,\n",
       "          1.0437,  0.6388, -0.3672, -2.9559,  0.0342, -0.4989,  1.0901, -0.0645,\n",
       "          0.0991,  0.4395, -0.9600,  0.2335,  0.2170, -0.9479,  0.2520,  0.8002,\n",
       "         -2.8480,  2.1689,  1.0637,  2.0842, -1.1276, -0.3783, -0.6248, -0.2147,\n",
       "         -1.4248, -0.5662, -0.7844,  0.6727,  0.6760,  0.7424, -3.7587, -1.4366,\n",
       "         -1.0418,  1.7875, -0.9385, -1.7401, -0.9464,  0.2496,  1.8283,  0.1886,\n",
       "         -0.8977, -0.8022, -0.3813, -0.1638, -0.6861,  0.9227, -0.4341,  1.8425,\n",
       "          1.1497, -0.2327,  1.8249,  0.3363,  0.2218,  0.3061, -2.4267,  1.9131,\n",
       "         -0.3945,  0.6016,  0.9345,  0.4843,  2.1649,  0.8584,  0.9726, -2.0890,\n",
       "         -0.4740, -2.8207,  3.0228,  1.4978, -0.5354,  0.0747,  0.6260,  0.2721,\n",
       "          1.2077,  1.0636,  0.5177, -0.5892, -1.3483,  0.6938,  1.3015,  0.9511,\n",
       "         -2.5351, -1.9060, -0.2154, -0.4941, -0.6678,  0.4963,  0.8723,  2.8967,\n",
       "          1.7197,  0.1618,  1.5567, -0.4081,  1.0546,  0.2344,  1.4652,  0.3772,\n",
       "         -0.8644, -0.8983,  0.0625,  0.3180,  4.6484,  1.1558, -0.9548,  0.8312,\n",
       "         -1.4711, -0.3189, -2.2446,  2.3359,  0.1609, -0.3596, -2.4999, -1.8392,\n",
       "          2.4574, -1.2779,  1.2877,  0.5722,  0.3823, -0.5596,  1.4857, -0.2390,\n",
       "         -0.3169,  1.6309,  1.3967,  1.3516,  1.8870, -0.9333,  1.7556,  1.7738,\n",
       "         -1.9920,  0.5248,  0.3724,  2.0121,  0.3689,  1.9977,  0.6154, -1.0482,\n",
       "          0.3450, -3.3204,  0.3683, -0.8336,  0.3991,  1.1257,  0.4047, -1.1230,\n",
       "          1.0185,  1.2507,  1.2463, -0.1254,  1.8235,  4.2746, -1.2293, -0.7417,\n",
       "         -1.5917, -1.5119,  0.2486, -0.9234, -2.0399, -1.3459,  1.2010, -1.6471,\n",
       "         -0.2182,  0.5591,  3.3888, -1.8817, -0.1747,  2.9160, -2.3193,  0.0776,\n",
       "          0.0771,  1.1013,  0.4903,  1.7470, -1.5159,  1.9787,  2.2410, -0.2282,\n",
       "          0.4927, -2.6284, -1.5590,  1.1259, -1.6140,  1.0871, -1.4689,  1.9711,\n",
       "         -1.2507,  1.3027,  0.4317,  0.6967, -1.4466,  0.1763,  0.7770, -0.5271,\n",
       "          0.0394, -2.2388, -1.5731,  1.1518,  0.7590,  0.4485,  3.5151, -0.9461,\n",
       "         -0.3462,  2.4412, -0.9962, -0.4066, -0.3408, -0.4289, -3.5031,  0.0610,\n",
       "          1.4183, -1.3626,  1.7539, -1.0400, -0.5382,  3.1473,  0.2098, -1.9749,\n",
       "          2.0457,  2.5787,  1.3869, -0.5311, -0.3930,  1.2327,  0.2003, -0.0687,\n",
       "          0.0622,  0.8009,  2.2023, -1.1207, -0.2550, -1.4107, -1.9707,  0.0654]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "qd_e = tt_a+tt_q\n",
    "qd_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# tt_d = model.q_embed_diff(tt)\n",
    "qd_e.view(-1,1,256).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out = model.model(tt_q.view(-1,1,256),qd_e.view(-1,1,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_q = torch.cat([test_out, tt_q.view(-1,1,256)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-0.1894]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "output = model.out(concat_q)\n",
    "output.reshape(-1)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "m = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0.4528]]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "m(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-4490], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "tt=test_qa_data[test_qa_data<0][0]\n",
    "tt = torch.Tensor([tt]).long().to(device)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model(tt_q,qd_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AKT(\n",
       "  (difficult_param): Embedding(914, 1)\n",
       "  (q_embed_diff): Embedding(9456, 256)\n",
       "  (qa_embed_diff): Embedding(18911, 256)\n",
       "  (q_embed): Embedding(9456, 256)\n",
       "  (qa_embed): Embedding(2, 256)\n",
       "  (model): Architecture(\n",
       "    (blocks_1): ModuleList(\n",
       "      (0): TransformerLayer(\n",
       "        (masked_attn_head): MultiHeadAttention(\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.05, inplace=False)\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (activation): ReLU()\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (blocks_2): ModuleList(\n",
       "      (0): TransformerLayer(\n",
       "        (masked_attn_head): MultiHeadAttention(\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.05, inplace=False)\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (activation): ReLU()\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (1): TransformerLayer(\n",
       "        (masked_attn_head): MultiHeadAttention(\n",
       "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.05, inplace=False)\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (activation): ReLU()\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.05, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.05, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from utils import model_isPid_type\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transpose_data_model = {'akt'}\n",
    "test_loss, test_accuracy, test_auc, all_pred = test(\n",
    "        model, args, None, test_q_data, test_qa_data, test_pid, label='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model_isPid_type' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-e6025434c7dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m all_pred = test(\n\u001b[0;32m----> 2\u001b[0;31m         model, args, None, test_q_data, test_qa_data, test_pid, label='Test')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-07dbb927690e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net, params, optimizer, q_data, qa_data, pid_data, label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# dataArray: [ array([[],[],..])] Shape: (3633, 200)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpid_flag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_isPid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_isPid_type' is not defined"
     ]
    }
   ],
   "source": [
    "all_pred = test(\n",
    "        model, args, None, test_q_data, test_qa_data, test_pid, label='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryEntropy(target, pred, mod=\"avg\"):\n",
    "    loss = target * np.log(np.maximum(1e-10, pred)) + \\\n",
    "        (1.0 - target) * np.log(np.maximum(1e-10, 1.0-pred))\n",
    "    if mod == 'avg':\n",
    "        return np.average(loss)*(-1.0)\n",
    "    elif mod == 'sum':\n",
    "        return - loss.sum()\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "\n",
    "def compute_auc(all_target, all_pred):\n",
    "    #fpr, tpr, thresholds = metrics.roc_curve(all_target, all_pred, pos_label=1.0)\n",
    "    return metrics.roc_auc_score(all_target, all_pred)\n",
    "\n",
    "\n",
    "def compute_accuracy(all_target, all_pred):\n",
    "    all_pred[all_pred > 0.5] = 1.0\n",
    "    all_pred[all_pred <= 0.5] = 0.0\n",
    "    return metrics.accuracy_score(all_target, all_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 197, 198, 199],\n",
       "        [  0,   1,   2,  ..., 197, 198, 199],\n",
       "        [  0,   1,   2,  ..., 197, 198, 199],\n",
       "        ...,\n",
       "        [  0,   1,   2,  ..., 197, 198, 199],\n",
       "        [  0,   1,   2,  ..., 197, 198, 199],\n",
       "        [  0,   1,   2,  ..., 197, 198, 199]])"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "# x1 = torch.arange(200)\n",
    "x1 = torch.arange(200).expand(200, -1)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,  ...,   0,   0,   0],\n",
       "        [  1,   1,   1,  ...,   1,   1,   1],\n",
       "        [  2,   2,   2,  ...,   2,   2,   2],\n",
       "        ...,\n",
       "        [197, 197, 197,  ..., 197, 197, 197],\n",
       "        [198, 198, 198,  ..., 198, 198, 198],\n",
       "        [199, 199, 199,  ..., 199, 199, 199]])"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "x2=x1.transpose(0,1)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,  ...,   0,   0,   0],\n",
       "        [  1,   1,   1,  ...,   1,   1,   1],\n",
       "        [  2,   2,   2,  ...,   2,   2,   2],\n",
       "        ...,\n",
       "        [197, 197, 197,  ..., 197, 197, 197],\n",
       "        [198, 198, 198,  ..., 198, 198, 198],\n",
       "        [199, 199, 199,  ..., 199, 199, 199]])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "x3=x2.contiguous()\n",
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TransformerLayer(\n",
       "  (masked_attn_head): MultiHeadAttention(\n",
       "    (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "    (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.05, inplace=False)\n",
       "  (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "  (activation): ReLU()\n",
       "  (dropout): Dropout(p=0.05, inplace=False)\n",
       "  (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout2): Dropout(p=0.05, inplace=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "model.model.blocks_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}